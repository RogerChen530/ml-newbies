{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附錄 A\n",
    "\n",
    "`pyvizml.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "__author__ = '{Yao-Jen Kuo}'\n",
    "__copyright__ = 'Copyright {2020}, {py-viz-ml-book}'\n",
    "__license__ = '{MIT}'\n",
    "__version__ = '{1}.{0}.{0}'\n",
    "__maintainer__ = '{Yao-Jen Kuo}'\n",
    "__email__ = '{tonykuoyj@gmail.com}'\n",
    "\n",
    "class CreateNBAData:\n",
    "    \"\"\"\n",
    "    This class scrapes NBA.com offical api: data.nba.net.\n",
    "    See https://data.nba.net/10s/prod/v1/today.json\n",
    "    Args:\n",
    "        season_year (int): Use the first year to specify season, e.g. specify 2019 for the 2019-2020 season.\n",
    "    \"\"\"\n",
    "    def __init__(self, season_year):\n",
    "        self._season_year = str(season_year)\n",
    "\n",
    "    def create_players_df(self):\n",
    "        \"\"\"\n",
    "        This function returns the DataFrame of player information.\n",
    "        \"\"\"\n",
    "        request_url = \"https://data.nba.net/prod/v1/{}/players.json\".format(self._season_year)\n",
    "        resp_dict = requests.get(request_url).json()\n",
    "        players_list = resp_dict['league']['standard']\n",
    "        players_list_dict = []\n",
    "        print(\"Creating players df...\")\n",
    "        for p in players_list:\n",
    "            player_dict = {}\n",
    "            for k, v in p.items():\n",
    "                if isinstance(v, str) or isinstance(v, bool):\n",
    "                    player_dict[k] = v\n",
    "            players_list_dict.append(player_dict)\n",
    "        df = pd.DataFrame(players_list_dict)\n",
    "        filtered_df = df[(df['isActive']) & (df['heightMeters'] != '')]\n",
    "        filtered_df = filtered_df.reset_index(drop=True)\n",
    "        self._person_ids = filtered_df['personId'].values\n",
    "        return filtered_df\n",
    "\n",
    "    def create_stats_df(self):\n",
    "        \"\"\"\n",
    "        This function returns the DataFrame of player career statistics.\n",
    "        \"\"\"\n",
    "        self.create_players_df()\n",
    "        career_summaries = []\n",
    "        print(\"Creating player stats df...\")\n",
    "        for pid in self._person_ids:\n",
    "            request_url = \"https://data.nba.net/prod/v1/{}/players/{}_profile.json\".format(self._season_year, pid)\n",
    "            response = requests.get(request_url)\n",
    "            profile_json = response.json()\n",
    "            career_summary = profile_json['league']['standard']['stats']['careerSummary']\n",
    "            career_summaries.append(career_summary)\n",
    "        stats_df = pd.DataFrame(career_summaries)\n",
    "        stats_df.insert(0, 'personId', self._person_ids)\n",
    "        return stats_df\n",
    "    \n",
    "    def create_player_stats_df(self):\n",
    "        \"\"\"\n",
    "        This function returns the DataFrame merged from players_df and stats_df.\n",
    "        \"\"\"\n",
    "        players = self.create_players_df()\n",
    "        stats = self.create_stats_df()\n",
    "        player_stats = pd.merge(players, stats, left_on='personId', right_on='personId')\n",
    "        return player_stats\n",
    "    \n",
    "class ImshowSubplots:\n",
    "    \"\"\"\n",
    "    This class plots 2d-arrays with subplots.\n",
    "    Args:\n",
    "        rows (int): The number of rows of axes.\n",
    "        cols (int): The number of columns of axes.\n",
    "        fig_size (tuple): Figure size.\n",
    "    \"\"\"\n",
    "    def __init__(self, rows, cols, fig_size):\n",
    "        self._rows = rows\n",
    "        self._cols = cols\n",
    "        self._fig_size = fig_size\n",
    "    def im_show(self, X, y, label_dict=None):\n",
    "        \"\"\"\n",
    "        This function plots 2d-arrays with subplots.\n",
    "        Args:\n",
    "            X (ndarray): 2d-arrays.\n",
    "            y (ndarray): Labels for 2d-arrays.\n",
    "            label_dict (dict): Str labels for y if any.\n",
    "        \"\"\"\n",
    "        n_pics = self._rows*self._cols\n",
    "        first_n_pics = X[:n_pics, :, :]\n",
    "        first_n_labels = y[:n_pics]\n",
    "        fig, axes = plt.subplots(self._rows, self._cols, figsize=self._fig_size)\n",
    "        for i in range(n_pics):\n",
    "            row_idx = i % self._rows\n",
    "            col_idx = i // self._rows\n",
    "            axes[row_idx, col_idx].imshow(first_n_pics[i], cmap=\"Greys\")\n",
    "            if label_dict is not None:\n",
    "                axes[row_idx, col_idx].set_title(\"Label: {}\".format(label_dict(first_n_labels[i])))\n",
    "            else:\n",
    "                axes[row_idx, col_idx].set_title(\"Label: {}\".format(first_n_labels[i]))\n",
    "            axes[row_idx, col_idx].set_xticks([])\n",
    "            axes[row_idx, col_idx].set_yticks([])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "class NormalEquation:\n",
    "    \"\"\"\n",
    "    This class defines the Normal equation for linear regression.\n",
    "    Args:\n",
    "        fit_intercept (bool): Whether to add intercept for this model.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self._fit_intercept = fit_intercept\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        This function uses Normal equation to solve for weights of this model.\n",
    "        Args:\n",
    "            X_train (ndarray): 2d-array for feature matrix of training data.\n",
    "            y_train (ndarray): 1d-array for target vector of training data.\n",
    "        \"\"\"\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        m = self._X_train.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_train = np.concatenate([X0, self._X_train], axis=1)\n",
    "        X_train_T = np.transpose(self._X_train)\n",
    "        left_matrix = np.dot(X_train_T, self._X_train)\n",
    "        right_matrix = np.dot(X_train_T, self._y_train)\n",
    "        left_matrix_inv = np.linalg.inv(left_matrix)\n",
    "        w = np.dot(left_matrix_inv, right_matrix)\n",
    "        w_ravel = w.ravel().copy()\n",
    "        self._w = w\n",
    "        self.intercept_ = w_ravel[0]\n",
    "        self.coef_ = w_ravel[1:]\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted values with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        self._X_test = X_test.copy()\n",
    "        m = self._X_test.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_test = np.concatenate([X0, self._X_test], axis=1)\n",
    "        y_pred = np.dot(self._X_test, self._w)\n",
    "        return y_pred\n",
    "    \n",
    "class GradientDescent:\n",
    "    \"\"\"\n",
    "    This class defines the vanilla gradient descent algorithm for linear regression.\n",
    "    Args:\n",
    "        fit_intercept (bool): Whether to add intercept for this model.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self._fit_intercept = fit_intercept\n",
    "    def find_gradient(self):\n",
    "        \"\"\"\n",
    "        This function returns the gradient given certain model weights.\n",
    "        \"\"\"\n",
    "        y_hat = np.dot(self._X_train, self._w)\n",
    "        gradient = (2/self._m) * np.dot(self._X_train.T, y_hat - self._y_train)\n",
    "        return gradient\n",
    "    def mean_squared_error(self):\n",
    "        \"\"\"\n",
    "        This function returns the mean squared error given certain model weights.\n",
    "        \"\"\"\n",
    "        y_hat = np.dot(self._X_train, self._w)\n",
    "        mse = ((y_hat - self._y_train).T.dot(y_hat - self._y_train)) / self._m\n",
    "        return mse\n",
    "    def fit(self, X_train, y_train, epochs=10000, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        This function uses vanilla gradient descent to solve for weights of this model.\n",
    "        Args:\n",
    "            X_train (ndarray): 2d-array for feature matrix of training data.\n",
    "            y_train (ndarray): 1d-array for target vector of training data.\n",
    "            epochs (int): The number of iterations to update the model weights.\n",
    "            learning_rate (float): The learning rate of gradient descent.\n",
    "        \"\"\"\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        self._m = self._X_train.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((self._m, 1), dtype=float)\n",
    "            self._X_train = np.concatenate([X0, self._X_train], axis=1)\n",
    "        n = self._X_train.shape[1]\n",
    "        self._w = np.random.rand(n)\n",
    "        n_prints = 10\n",
    "        print_iter = epochs // n_prints\n",
    "        w_history = dict()\n",
    "        for i in range(epochs):\n",
    "            current_w = self._w.copy()\n",
    "            w_history[i] = current_w\n",
    "            mse = self.mean_squared_error()\n",
    "            gradient = self.find_gradient()\n",
    "            if i % print_iter == 0:\n",
    "                print(\"epoch: {:6} - loss: {:.6f}\".format(i, mse))\n",
    "            self._w -= learning_rate*gradient\n",
    "        w_ravel = self._w.copy().ravel()\n",
    "        self.intercept_ = w_ravel[0]\n",
    "        self.coef_ = w_ravel[1:]\n",
    "        self._w_history = w_history\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted values with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        self._X_test = X_test\n",
    "        m = self._X_test.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_test = np.concatenate([X0, self._X_test], axis=1)\n",
    "        y_pred = np.dot(self._X_test, self._w)\n",
    "        return y_pred\n",
    "    \n",
    "class AdaGrad(GradientDescent):\n",
    "    \"\"\"\n",
    "    This class defines the Adaptive Gradient Descent algorithm for linear regression.\n",
    "    \"\"\"\n",
    "    def fit(self, X_train, y_train, epochs=10000, learning_rate=0.01, epsilon=1e-06):\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        self._m = self._X_train.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((self._m, 1), dtype=float)\n",
    "            self._X_train = np.concatenate([X0, self._X_train], axis=1)\n",
    "        n = self._X_train.shape[1]\n",
    "        self._w = np.random.rand(n)\n",
    "        # 初始化 ssg\n",
    "        ssg = np.zeros(n, dtype=float)\n",
    "        n_prints = 10\n",
    "        print_iter = epochs // n_prints\n",
    "        w_history = dict()\n",
    "        for i in range(epochs):\n",
    "            current_w = self._w.copy()\n",
    "            w_history[i] = current_w\n",
    "            mse = self.mean_squared_error()\n",
    "            gradient = self.find_gradient()\n",
    "            ssg += gradient**2\n",
    "            ada_grad = gradient / (epsilon + ssg**0.5)\n",
    "            if i % print_iter == 0:\n",
    "                print(\"epoch: {:6} - loss: {:.6f}\".format(i, mse))\n",
    "            # 以 adaptive gradient 更新 w\n",
    "            self._w -= learning_rate*ada_grad\n",
    "        w_ravel = self._w.copy().ravel()\n",
    "        self.intercept_ = w_ravel[0]\n",
    "        self.coef_ = w_ravel[1:]\n",
    "        self._w_history = w_history\n",
    "        \n",
    "class LogitReg:\n",
    "    \"\"\"\n",
    "    This class defines the vanilla descent algorithm for logistic regression.\n",
    "    Args:\n",
    "        fit_intercept (bool): Whether to add intercept for this model.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self._fit_intercept = fit_intercept\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        This function returns the Sigmoid output as a probability given certain model weights.\n",
    "        \"\"\"\n",
    "        X_w = np.dot(X, self._w)\n",
    "        p_hat = 1 / (1 + np.exp(-X_w))\n",
    "        return p_hat\n",
    "    def find_gradient(self):\n",
    "        \"\"\"\n",
    "        This function returns the gradient given certain model weights.\n",
    "        \"\"\"\n",
    "        m = self._m\n",
    "        p_hat = self.sigmoid(self._X_train)\n",
    "        X_train_T = np.transpose(self._X_train)\n",
    "        gradient = (1/m) * np.dot(X_train_T, p_hat - self._y_train)\n",
    "        return gradient\n",
    "    def cross_entropy(self, epsilon=1e-06):\n",
    "        \"\"\"\n",
    "        This function returns the cross entropy given certain model weights.\n",
    "        \"\"\"\n",
    "        m = self._m\n",
    "        p_hat = self.sigmoid(self._X_train)\n",
    "        cost_y1 = -np.dot(self._y_train, np.log(p_hat + epsilon))\n",
    "        cost_y0 = -np.dot(1 - self._y_train, np.log(1 - p_hat + epsilon))\n",
    "        cross_entropy = (cost_y1 + cost_y0) / m\n",
    "        return cross_entropy\n",
    "    def fit(self, X_train, y_train, epochs=10000, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        This function uses vanilla gradient descent to solve for weights of this model.\n",
    "        Args:\n",
    "            X_train (ndarray): 2d-array for feature matrix of training data.\n",
    "            y_train (ndarray): 1d-array for target vector of training data.\n",
    "            epochs (int): The number of iterations to update the model weights.\n",
    "            learning_rate (float): The learning rate of gradient descent.\n",
    "        \"\"\"\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        m = self._X_train.shape[0]\n",
    "        self._m = m\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((self._m, 1), dtype=float)\n",
    "            self._X_train = np.concatenate([X0, self._X_train], axis=1)\n",
    "        n = self._X_train.shape[1]\n",
    "        self._w = np.random.rand(n)\n",
    "        n_prints = 10\n",
    "        print_iter = epochs // n_prints\n",
    "        for i in range(epochs):\n",
    "            cross_entropy = self.cross_entropy()\n",
    "            gradient = self.find_gradient()\n",
    "            if i % print_iter == 0:\n",
    "                print(\"epoch: {:6} - loss: {:.6f}\".format(i, cross_entropy))\n",
    "            self._w -= learning_rate*gradient\n",
    "        w_ravel = self._w.ravel().copy()\n",
    "        self.intercept_ = w_ravel[0]\n",
    "        self.coef_ = w_ravel[1:].reshape(1, -1)\n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted probability with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        m = X_test.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_test = np.concatenate([X0, X_test], axis=1)\n",
    "        p_hat_1 = self.sigmoid(self._X_test).reshape(-1, 1)\n",
    "        p_hat_0 = 1 - p_hat_1\n",
    "        proba = np.concatenate([p_hat_0, p_hat_1], axis=1)\n",
    "        return proba\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted label with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X_test)\n",
    "        y_pred = np.argmax(proba, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "class ClfMetrics:\n",
    "    \"\"\"\n",
    "    This class calculates some of the metrics of classifier including accuracy, precision, recall, f1 according to confusion matrix.\n",
    "    Args:\n",
    "        y_true (ndarray): 1d-array for true target vector.\n",
    "        y_pred (ndarray): 1d-array for predicted target vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self._y_true = y_true\n",
    "        self._y_pred = y_pred\n",
    "    def confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        This function returns the confusion matrix given true/predicted target vectors.\n",
    "        \"\"\"\n",
    "        n_unique = np.unique(self._y_true).size\n",
    "        cm = np.zeros((n_unique, n_unique), dtype=int)\n",
    "        for i in range(n_unique):\n",
    "            for j in range(n_unique):\n",
    "                n_obs = np.sum(np.logical_and(self._y_true == i, self._y_pred == j))\n",
    "                cm[i, j] = n_obs\n",
    "        self._tn = cm[0, 0]\n",
    "        self._tp = cm[1, 1]\n",
    "        self._fn = cm[0, 1]\n",
    "        self._fp = cm[1, 0]\n",
    "        return cm\n",
    "    def accuracy_score(self):\n",
    "        \"\"\"\n",
    "        This function returns the accuracy score given true/predicted target vectors.\n",
    "        \"\"\"\n",
    "        cm = self.confusion_matrix()\n",
    "        accuracy = (self._tn + self._tp) / np.sum(cm)\n",
    "        return accuracy\n",
    "    def precision_score(self):\n",
    "        \"\"\"\n",
    "        This function returns the precision score given true/predicted target vectors.\n",
    "        \"\"\"\n",
    "        precision = self._tp / (self._tp + self._fp)\n",
    "        return precision  \n",
    "    def recall_score(self):\n",
    "        \"\"\"\n",
    "        This function returns the recall score given true/predicted target vectors.\n",
    "        \"\"\"\n",
    "        recall = self._tp / (self._tp + self._fn)\n",
    "        return recall\n",
    "    def f1_score(self, beta=1):\n",
    "        \"\"\"\n",
    "        This function returns the f1 score given true/predicted target vectors.\n",
    "        Args:\n",
    "            beta (int, float): Can be used to generalize from f1 score to f score.\n",
    "        \"\"\"\n",
    "        precision = self.precision_score()\n",
    "        recall = self.recall_score()\n",
    "        f1 = (1 + beta**2)*precision*recall / ((beta**2 * precision) + recall)\n",
    "        return f1\n",
    "\n",
    "class DeepLearning:\n",
    "    \"\"\"\n",
    "    This class defines the vanilla optimization of a deep learning model.\n",
    "    Args:\n",
    "        layer_of_units (list): A list to specify the number of units in each layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_of_units):\n",
    "        self._n_layers = len(layer_of_units)\n",
    "        parameters = {}\n",
    "        for i in range(self._n_layers - 1):\n",
    "            parameters['W{}'.format(i + 1)] = np.random.rand(layer_of_units[i + 1], layer_of_units[i])\n",
    "            parameters['B{}'.format(i + 1)] = np.random.rand(layer_of_units[i + 1], 1)\n",
    "        self._parameters = parameters\n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        This function returns the Sigmoid output.\n",
    "        Args:\n",
    "            Z (ndarray): The multiplication of weights and output from previous layer.\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "    def single_layer_forward_propagation(self, A_previous, W_current, B_current):\n",
    "        \"\"\"\n",
    "        This function returns the output of a single layer of forward propagation.\n",
    "        Args:\n",
    "            A_previous (ndarray): The Sigmoid output from previous layer.\n",
    "            W_current (ndarray): The weights of current layer.\n",
    "            B_current (ndarray): The bias of current layer.\n",
    "        \"\"\"\n",
    "        Z_current = np.dot(W_current, A_previous) + B_current\n",
    "        A_current = self.sigmoid(Z_current)\n",
    "        return A_current, Z_current\n",
    "    def forward_propagation(self):\n",
    "        \"\"\"\n",
    "        This function returns the output of a complete round of forward propagation.\n",
    "        \"\"\"\n",
    "        self._m = self._X_train.shape[0]\n",
    "        X_train_T = self._X_train.copy().T\n",
    "        cache = {}\n",
    "        A_current = X_train_T\n",
    "        for i in range(self._n_layers - 1):\n",
    "            A_previous = A_current\n",
    "            W_current = self._parameters[\"W{}\".format(i + 1)]\n",
    "            B_current = self._parameters[\"B{}\".format(i + 1)]\n",
    "            A_current, Z_current = self.single_layer_forward_propagation(A_previous, W_current, B_current)\n",
    "            cache[\"A{}\".format(i)] = A_previous\n",
    "            cache[\"Z{}\".format(i + 1)] = Z_current\n",
    "        self._cache = cache\n",
    "        self._A_current = A_current\n",
    "    def derivative_sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        This function returns the output of the derivative of Sigmoid function.\n",
    "        Args:\n",
    "            Z (ndarray): The multiplication of weights, bias and output from previous layer.\n",
    "        \"\"\"\n",
    "        sig = self.sigmoid(Z)\n",
    "        return sig * (1 - sig)\n",
    "    def single_layer_backward_propagation(self, dA_current, W_current, B_current, Z_current, A_previous):\n",
    "        \"\"\"\n",
    "        This function returns the output of a single layer of backward propagation.\n",
    "        Args:\n",
    "            dA_current (ndarray): The output of the derivative of Sigmoid function from previous layer.\n",
    "            W_current (ndarray): The weights of current layer.\n",
    "            B_current (ndarray): The bias of current layer.\n",
    "            Z_current (ndarray): The multiplication of weights, bias and output from previous layer.\n",
    "            A_previous (ndarray): The Sigmoid output from previous layer.\n",
    "        \"\"\"\n",
    "        dZ_current = dA_current * self.derivative_sigmoid(Z_current)\n",
    "        dW_current = np.dot(dZ_current, A_previous.T) / self._m\n",
    "        dB_current = np.sum(dZ_current, axis=1, keepdims=True) / self._m\n",
    "        dA_previous = np.dot(W_current.T, dZ_current)\n",
    "        return dA_previous, dW_current, dB_current\n",
    "    def backward_propagation(self):\n",
    "        \"\"\"\n",
    "        This function performs a complete round of backward propagation to update weights and bias.\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        self.forward_propagation()\n",
    "        Y_hat = self._A_current.copy()\n",
    "        Y_train = self._y_train.copy().reshape(1, self._m)\n",
    "        dA_previous = - (np.divide(Y_train, Y_hat) - np.divide(1 - Y_train, 1 - Y_hat))\n",
    "        for i in reversed(range(dl._n_layers - 1)):\n",
    "            dA_current = dA_previous\n",
    "            A_previous = self._cache[\"A{}\".format(i)]\n",
    "            Z_current = self._cache[\"Z{}\".format(i+1)]\n",
    "            W_current = self._parameters[\"W{}\".format(i+1)]\n",
    "            B_current = self._parameters[\"B{}\".format(i+1)]\n",
    "            dA_previous, dW_current, dB_current = self.single_layer_backward_propagation(dA_current, W_current, B_current, Z_current, A_previous)\n",
    "            gradients[\"dW{}\".format(i + 1)] = dW_current\n",
    "            gradients[\"dB{}\".format(i + 1)] = dB_current\n",
    "        self._gradients = gradients\n",
    "    def cross_entropy(self):\n",
    "        \"\"\"\n",
    "        This function returns the cross entropy given weights and bias.\n",
    "        \"\"\"\n",
    "        Y_hat = self._A_current.copy()\n",
    "        self._Y_hat = Y_hat\n",
    "        Y_train = self._y_train.copy().reshape(1, self._m)\n",
    "        ce = -1 / self._m * (np.dot(Y_train, np.log(Y_hat).T) + np.dot(1 - Y_train, np.log(1 - Y_hat).T))\n",
    "        return ce[0, 0]\n",
    "    def accuracy_score(self):\n",
    "        \"\"\"\n",
    "        This function returns the accuracy score given weights and bias.\n",
    "        \"\"\"\n",
    "        p_pred = self._Y_hat.ravel()\n",
    "        y_pred = np.where(p_pred > 0.5, 1, 0)\n",
    "        y_true = self._y_train\n",
    "        accuracy = (y_pred == y_true).sum() / y_pred.size\n",
    "        return accuracy\n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        This function performs vanilla gradient descent to update weights and bias.\n",
    "        \"\"\"\n",
    "        for i in range(self._n_layers - 1):\n",
    "            self._parameters[\"W{}\".format(i + 1)] -= self._learning_rate * self._gradients[\"dW{}\".format(i + 1)]\n",
    "            self._parameters[\"B{}\".format(i + 1)] -= self._learning_rate * self._gradients[\"dB{}\".format(i + 1)]\n",
    "    def fit(self, X_train, y_train, epochs=100000, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        This function uses multiple rounds of forward propagations and backward propagations to optimize weights and bias.\n",
    "        Args:\n",
    "            X_train (ndarray): 2d-array for feature matrix of training data.\n",
    "            y_train (ndarray): 1d-array for target vector of training data.\n",
    "            epochs (int): The number of iterations to update the model weights.\n",
    "            learning_rate (float): The learning rate of gradient descent.\n",
    "        \"\"\"\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        self._learning_rate = learning_rate\n",
    "        loss_history = []\n",
    "        accuracy_history = []\n",
    "        n_prints = 10\n",
    "        print_iter = epochs // n_prints\n",
    "        for i in range(epochs):\n",
    "            self.forward_propagation()\n",
    "            ce = self.cross_entropy()\n",
    "            accuracy = self.accuracy_score()\n",
    "            loss_history.append(ce)\n",
    "            accuracy_history.append(accuracy)\n",
    "            self.backward_propagation()\n",
    "            self.gradient_descent()\n",
    "            if i % print_iter == 0:\n",
    "                print(\"Iteration: {:6} - cost: {:.6f} - accuracy: {:.2f}%\".format(i, ce, accuracy * 100))\n",
    "        self._loss_history = loss_history\n",
    "        self._accuracy_history = accuracy_history\n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted probability for class 1 with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        X_test_T = X_test.copy().T\n",
    "        A_current = X_test_T\n",
    "        for i in range(self._n_layers - 1):\n",
    "            A_previous = A_current\n",
    "            W_current = self._parameters[\"W{}\".format(i + 1)]\n",
    "            B_current = self._parameters[\"B{}\".format(i + 1)]\n",
    "            A_current, Z_current = self.single_layer_forward_propagation(A_previous, W_current, B_current)\n",
    "            self._cache[\"A{}\".format(i)] = A_previous\n",
    "            self._cache[\"Z{}\".format(i + 1)] = Z_current\n",
    "        p_hat_1 = A_current.copy().ravel()\n",
    "        return p_hat_1\n",
    "    def predict(self, X_test):\n",
    "        p_hat_1 = self.predict_proba(X_test)\n",
    "        return np.where(p_hat_1 >= 0.5, 1, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Machine Learning",
   "language": "python",
   "name": "pyml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
